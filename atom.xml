<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Eric Rochester</title>
    <link href="http://www.ericrochester.com//atom.xml" rel="self" />
    <link href="http://www.ericrochester.com/" />
    <id>http://www.ericrochester.com//atom.xml</id>
    <author>
        <name>Eric Rochester</name>
        <email>erochest@gmail.com</email>
    </author>
    <updated>2014-02-27T15:00:00Z</updated>
    <entry>
    <title>Software Development for the MA Humanities Student</title>
    <link href="http://www.ericrochester.com//pages/alt-ac/software-ma-humanities-student/index.html" />
    <id>http://www.ericrochester.com//pages/alt-ac/software-ma-humanities-student/index.html</id>
    <published>2014-02-27T15:00:00Z</published>
    <updated>2014-02-27T15:00:00Z</updated>
    <summary type="html"><![CDATA[<blockquote>
<p>This was originally posted on <a href="http://www.scholarslab.org/uncategorized/software-development-for-the-ma-humanities-student/">the Scholars’ Lab blog</a>. I’ve cross posted it here.</p>
</blockquote>
<p>
This is <em>not</em> a transcript of a brief panel talk I gave for the UVa <a href="http://graduate.engl.virginia.edu/gesa/">Graduate English Student Association</a> Career Panel. It’s based on what I hope to say, but I’m actually writing this before the event so it (and its links) can be available beforehand.
</p>

<h1 id="about-me">
About me
</h1>

<p>
I’ve been interested in two things for about as long as I can remember: computers and literature. These intersected a little in science fiction and fantasy, but largely, the two obsessions remained strangely separate. I’d spent a lot of time reading, both “literature” and “trash”; but I’d also enjoyed playing computer games and trying to program my own.
</p>

<p>
It wasn’t until half-way through my PhD program at <a
href="http://www.uga.edu/">The University of Georgia</a> that my interests started to come together. Initially, I just had a reputation for being able to help people format columns in Word. Then I got involved in digital humanities, then called humanities computing. I also created a website for a professor, and later I started doing web development and systems administration for the <a
href="http://www.lap.uga.edu/">Linguistic Atlas Projects</a>.
</p>

<p>
Although these jobs weren’t my primary focus in graduate school, I did take them seriously. I learned best practices, including version control and testing. This was good for the project, but it was also good for me: doing things right up front saved me pain and sweat later.
</p>

<p>
And this was how my two interests finally found common ground.
</p>

<p>
When I graduated I took a job doing a combination of corpus linguistics and software development. This was good, but when I needed to look for another job, I found that there were fewer options for corpus linguistics than for web development.
</p>

<p>
So I made web sites for a few years. I had a lot of fun, and I learned a lot, both about the work itself and about interacting with clients and stakeholders.
</p>

<p>
For the last almost three years, I’ve been senior developer here at the <a
href="http://www.scholarslab.org/">Scholars’ Lab</a>. What does that entail?
</p>

<ul>

<li>
<strong>Software development</strong>: True to my title, a lot of what I do involves developing and maintaining computer systems and web sites.
</li>

<li>
<strong>Mentoring and education</strong>: Our biggest focus is education and mentoring. Sometimes that means helping someone who walks in with a digital project. More often it involves helping one of the Scholars’ Lab’s fellows or one of the students in the <a href="http://praxis.scholarslab.org/">Praxis Program</a>.
</li>

<li>
<strong>Documentation</strong>: An important—but often overlooked—aspect of software projects is their documentation. We don’t spend enough time on this.
</li>

</ul>

<p>
That’s not all, but those three is probably how I spend most of my time.
</p>

<h1 id="what-kind-of-work-are-we-talking-about">
What Kind of Work are we Talking about?
</h1>

<p>
I’m a software developer, so I’ve necessarily focused on that in talking about my personal journey. However, software projects are large, sprawling, complex behemoths, and there are a lot of different tasks that need to be done and a lot of different specialties that are required to contribute. So even if writing code doesn’t appeal to you, other things might.
</p>

<ul>

<li>
<strong>Project management</strong>: Keep everyone on track.
</li>

<li>
<strong>Community outreach</strong>: Publicize the project and be an active member of the project’s community.
</li>

<li>
<strong>Design</strong>: Make the product usable.
</li>

<li>
<strong>Documentation</strong>: A different way to make the product usable.
</li>

<li>
<strong>Testing</strong>: Check that the product works and works correctly.
</li>

</ul>

<blockquote> <p>
Gina Trapani has an excellent post talking about how crucial—but also how under-valued—many tasks are in a software project, especially in the open source world. You can read about it at <a
href="http://smarterware.org/7550/designers-women-and-hostility-in-open-source">Designers, Women, and Hostility in Open Source</a>.
</p> </blockquote>

<h1 id="what-advantages-do-you-have">
What Advantages do you Have?
</h1>

<p>
Typically, people expect those in any technical job to have a STEM background. This is false, and in fact, a humanities background can be a great asset in almost any job in software development.
</p>

<p>
Let me count the ways.
</p>

<ul>

<li>
<strong>Communication</strong> This point is trite, but it’s true. At a fundamental level programming involves communicating. Your code must communicate to the computer, to other developers, and even to your future self. You’ll also need to communicate effectively to clients, to your boss, and to co-workers.
</li>

<li>
<strong>Education and Negotiation</strong> An important part of software development involves educating and negotiating with others. For example, adding a feature may involve dropping another one. This doesn’t make clients happy, and you’ll need to explain why and argue your case.
</li>

<li>
<strong>Research</strong> Learning new technologies as well as using ones you’re already familiar with both involve a lot of research. Knowing how to learn and how to research is an important asset here.
</li>

<li>
<strong>Reading and interpretation</strong> Most programmers work from specification documents. Being able to interpret them appropriately is crucial.
</li>

<li>
<strong>Multi-level focus</strong> I’m not sure what to call this point, but it may be the most important one. When you analyze literature you must command details from a variety of texts and sources and synthesize them to make a larger point. This involves paying attention to both the forest and the trees. Writing software involves the same split focus: on the one hand, you spend a lot of time in the weeds thinking about semicolons; on the other hand, you must keep the big picture in mind to stay on track and on schedule.
</li>

</ul>

<blockquote> <p>
For some of the same points, plus some others, see Shelby Switzer’s post on <a
href="http://shelbyswitzer.com/humanities_degrees_help_programmers/">How my “impractical” humanities degree prepared me for a career in programming</a>.
</p> </blockquote>

<h1 id="what-can-you-be-doing">
What Can you be Doing Now?
</h1>

<p>
Obviously, finish your degree. This is the most important thing you can do.
</p>

<p>
But in your spare time (ha!), there are some other things you can do, both now and in the future. (Again, apologies: this list is for software programmers, especially web developers.)
</p>

<ul>

<li>
<strong>Learn the basics</strong> For web development, design, etc., this means learning HTML, CSS, and JavaScript. <a
href="http://www.codecademy.com/">CodeAcademy</a>, <a
href="https://www.codeschool.com/">Code School</a>, and <a
href="http://code.tutsplus.com/">tuts+</a> are all good. The main thing is to type along yourself.
</li>

<li>
<strong>Learn a Web Language</strong> Essentially, you want something you can use to interact with a database and dynamically generate web pages. This could be <a href="http://nodejs.org/">JavaScript using NodeJS</a>, <a
href="https://www.ruby-lang.org/">Ruby</a>, or <a
href="http://www.python.org/">Python</a>. Any of these are good. If there’s one available for your language of choice, the <a
href="http://learncodethehardway.org/">Learn Code the Hard Way</a> series is excellent.
</li>

<li>
<strong>Learn a Web Framework</strong> Find one based on the language you picked in the previous point. For JavaScript, that means <a
href="http://expressjs.com/">Express</a> or <a
href="http://sailsjs.org/">Sails</a>. For Ruby, <a
href="http://rubyonrails.org/">Ruby on Rails</a> or <a
href="http://www.sinatrarb.com/">Sinatra</a>. For Python, <a
href="https://www.djangoproject.com/">Django</a> or <a
href="http://flask.pocoo.org/">Flask</a>.
</li>

</ul>

<p>
If you’re just getting started, don’t worry about getting a broad knowledge of different technologies. All of them are similar. You’ll be better served by going deep into one choice. What you learn will apply to the other systems, and you can learn them later when required.
</p>

<p>
Also, learn the tools you’ll use to work in these languages. Learn them thoroughly and learn them well. You’re going to live in them.
</p>

<ul>

<li>
<strong>A text editor</strong> This is probably the single-most important tool for a software developer. Know it inside and out. Know all of its tricks. <a href="http://www.sublimetext.com/">Sublime Text</a> is a popular choice right now.
</li>

<li>
<strong>Version control</strong> Programmers use version control systems to track the changes they make to their code. <a
href="http://git-scm.com/">Git</a> is a very popular choice, and <a
href="https://github.com/">Github</a> allows you to share your code and collaborate with others.
</li>

<li>
<strong>Online documentation</strong> Find the documentation for your programming language, its libraries, and the web framework you’re using. Also <a href="http://stackoverflow.com/">StackOverflow</a> is a popular site for asking questions related to software development.
</li>

</ul>

<p>
Finally, <strong>get your work out there</strong>. There’s never been a better time for this than right now. You can put your code online for others to see on <a href="https://github.com/">Github</a>. You can also run web apps quickly and easily using <a href="http://www.heroku.com/">Heroku</a>. Having code up on these makes it easy for potential employers to see your skills. It also lets them know that you’re active and learning and capable. They won’t replace a good portfolio that directs potential employers’ attention and highlights your best work, but they are a good start, and they’ll set up above most other applicants.
</p>

<p>
In general, this is a great time to go into software development and other technical jobs. Hopefully this post tells you what you need to think about and plan for.
</p>

]]></summary>
</entry>
<entry>
    <title>A Simple Dataflow System</title>
    <link href="http://www.ericrochester.com//pages/code/a-simple-dataflow-system/index.html" />
    <id>http://www.ericrochester.com//pages/code/a-simple-dataflow-system/index.html</id>
    <published>2013-06-05T14:16:02Z</published>
    <updated>2013-06-05T14:16:02Z</updated>
    <summary type="html"><![CDATA[<blockquote>
<p><em>This is a recipe that I wrote for the <a href="/pages/announcements/clj-data-analysis/index.html"><em>Clojure Data Analysis Cookbook</em></a>. However, it didn’t make it into the final book, so I’m sharing it with you today.</em></p>
</blockquote>
<p>When working with data, it’s often useful to have a computer language or DSL that allows us to express how data flows through our program. The computer can then decide how best to execute that flow, whether it should be spread across multiple cores or even multiple machines.</p>
<p>This style of programming is called <a href="http://en.wikipedia.org/wiki/Dataflow_programming"><strong>dataflow programming</strong></a>. There are a couple of different ways of looking at dataflow programming. One way describes it as being like a spreadsheet. We declare relationships between cells, and a change in one cell percolates through the graph.</p>
<p>Another way of looking at it is as a graph that captures the computation. Each computation is a node, and the data flows between them. After we build the computation graph, we tell the computer to run it how it seems best. It could distribute the computations across cores or even across computers in a network.</p>
<p>And guess what? Clojure itself allows that kind of expression, especially with the threading macros (<code>-&gt;</code> and <code>-&gt;&gt;</code>). Reducers handle parallelization under the covers. Let’s see how far we can take that.</p>
<h3 id="getting-ready">Getting ready…</h3>
<p>To use reducers, we first need to depend on Clojure 1.5. We’ll need this in our <a href="http://leiningen.org/">Leiningen</a> <a href="project.clj"><code>project.clj</code></a> file:</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure"><span class="kw">:dependencies</span> [[org.clojure/clojure <span class="st">&quot;1.5.1&quot;</span>]]</code></pre>
<p>We also need to import the library, and since it defines several functions with the same name as core functions, we’ll alias it.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">require</span> &#39;[clojure.core.reducers <span class="kw">:as</span> r])</code></pre>
<p>For this example, we’ll work with a list of the Doctor’s companions from the entire run of Doctor Who. I won’t reproduce the whole list, but here is a sample of six:</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">def</span><span class="fu"> input-data</span>
  [{<span class="kw">:given-name</span> <span class="st">&quot;Susan&quot;</span>, <span class="kw">:surname</span> <span class="st">&quot;Forman&quot;</span>,
    <span class="kw">:doctors</span> [<span class="dv">1</span>]}
   {<span class="kw">:given-name</span> <span class="st">&quot;Harry&quot;</span>, <span class="kw">:surname</span> <span class="st">&quot;Sullivan&quot;</span>,
    <span class="kw">:doctors</span> [<span class="dv">4</span>]}
   {<span class="kw">:given-name</span> <span class="st">&quot;Nyssa&quot;</span>, <span class="kw">:surname</span> nil,
    <span class="kw">:doctors</span> [<span class="dv">4</span> <span class="dv">5</span>]}
   {<span class="kw">:given-name</span> <span class="st">&quot;Melanie&quot;</span>, <span class="kw">:surname</span> <span class="st">&quot;Bush&quot;</span>,
    <span class="kw">:doctors</span> [<span class="dv">6</span> <span class="dv">7</span>]}
   {<span class="kw">:given-name</span> <span class="st">&quot;Jackson&quot;</span>, <span class="kw">:surname</span> <span class="st">&quot;Lake&quot;</span>,
    <span class="kw">:doctors</span> [<span class="dv">10</span>]}
   {<span class="kw">:given-name</span> <span class="st">&quot;Craig&quot;</span>, <span class="kw">:surname</span> <span class="st">&quot;Owens&quot;</span>,
    <span class="kw">:doctors</span> [<span class="dv">11</span>]}])</code></pre>
<p>(If you want to use the entire dataset, you can download it from the book’s <a href="/clj-data-analysis/">data page</a> or directly from <a href="/clj-data-analysis/data/companions.clj">this link</a>.)</p>
<h3 id="how-to-do-it">How to do it…</h3>
<p>For our slightly contrived example, we’ll compute the average length of the companions’ surnames, for those who have surnames. First, we’ll need a couple of functions to accumulate the item count and sum for computing the mean. We’ll also need a function to add two accumulators’ data together. And we’ll need a function to calculate the mean from an accumulator’s data. Here are those functions.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> accum-mean</span>
  <span class="st">&quot;Accumulate the sum and length of a sequence for</span>
<span class="st">  calculating the mean.&quot;</span>
  ([] {<span class="kw">:sum</span> <span class="dv">0</span>, <span class="kw">:n</span> <span class="dv">0</span>})
  ([{<span class="kw">:keys</span> [sum n]} x]
   {<span class="kw">:sum</span> (<span class="kw">+</span> sum x)
    <span class="kw">:n</span> (<span class="kw">inc</span> n)}))
(<span class="kw">defn</span><span class="fu"> join-accum</span>
  <span class="st">&quot;Take the output of two calls to accum-mean and join</span>
<span class="st">  them.&quot;</span>
  ([] {<span class="kw">:sum</span> <span class="dv">0</span>, <span class="kw">:n</span> <span class="dv">0</span>})
  ([accum1 accum2]
   {<span class="kw">:sum</span> (<span class="kw">+</span> (<span class="kw">:sum</span> accum1) (<span class="kw">:sum</span> accum2))
    <span class="kw">:n</span> (<span class="kw">+</span> (<span class="kw">:n</span> accum1) (<span class="kw">:n</span> accum2))}))
(<span class="kw">defn</span><span class="fu"> calc-mean</span>
  <span class="st">&quot;Take the output of accum-mean or join-accum and</span>
<span class="st">  calculate the mean.&quot;</span>
  [{<span class="kw">:keys</span> [sum n]}]
  (<span class="kw">double</span> (<span class="kw">/</span> sum n)))</code></pre>
<p>With these in place, we can define a function that creates a reducer that returns the length of the surnames, filtering out those with no surname. Combined with a threading macro, this makes a very clear dataflow.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> process-seq</span>
  [coll]
  (<span class="kw">-&gt;&gt;</span>
    coll
    (r/map <span class="kw">:surname</span>)
    (r/filter #(<span class="kw">not</span> (<span class="kw">nil?</span> %)))
    (r/map <span class="kw">count</span>)))</code></pre>
<p>First we can run that with the core reduce function to execute it sequentially.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">user=&gt; (calc-mean (<span class="kw">reduce</span> accum-mean (accum-mean) 
                          (process-seq input-data)))
<span class="fl">5.4</span></code></pre>
<p>But by changing from reduce to <a href="http://clojure.github.io/clojure/clojure.core-api.html#clojure.core.reducers/fold"><code>clojure.core.reducers/fold</code></a>, it will automatically partition our data and spread the processing across multiple cores, even given the same input process.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">user=&gt; (calc-mean (r/fold join-accum accum-mean 
                          (process-seq input-data)))
<span class="fl">5.4</span></code></pre>
<h3 id="how-it-works">How it works…</h3>
<p>Because it has a more complicated execution model, the <code>r/fold</code> function takes a little more information.</p>
<ol style="list-style-type: decimal">
<li><p>The <code>accum-mean</code> function takes the results of the process and turns them into an accumulator map.</p></li>
<li><p>But if <code>r/fold</code> decides to use more than one partition, those maps will need to be combined. That’s where <code>join-accum</code> comes into play.</p></li>
</ol>
<p>Both the reducer and combiner functions can be called with no parameters. In that case, both return a zero accumulator map. We use that with the <code>reduce</code> call, and <code>r/fold</code> will use it in its processing to get the starting state for the reduce and combine steps.</p>
<p>In either case, reducers allow us to create a data structure for processing the data and decide later—or let the computer decide—how best to execute the process. Meanwhile, our code remains clear and readable, and what is happening with the data is obvious. <!-- vim: set textwidth=58: --></p>]]></summary>
</entry>
<entry>
    <title>Parallel IO with mmap</title>
    <link href="http://www.ericrochester.com//pages/code/parallel-io-with-mmap/index.html" />
    <id>http://www.ericrochester.com//pages/code/parallel-io-with-mmap/index.html</id>
    <published>2013-05-22T13:33:00Z</published>
    <updated>2013-05-22T13:33:00Z</updated>
    <summary type="html"><![CDATA[<blockquote>
<p><em>This is a recipe that I wrote for the <a href="/pages/announcements/clj-data-analysis/index.html"><em>Clojure Data Analysis Cookbook</em></a>. However, it didn’t make it into the final book, so I’m sharing it with you today.</em></p>
</blockquote>
<p>Parallelizing the pure parts of processes, the parts that don’t involve side effects like reading input files, is relatively easy. But once disk IO enters the picture, things become more complicated. There’s a reason for that: when reading from a disk, all threads are inherently contending for one resource, the disk.</p>
<p>There are ways to mitigate this, but ultimately it comes down to working with the disk and the processing requirements to get the best performance we can from a single, sequential process. So to be completely honest, the title of this post is a little misleading. We can’t parallelize IO on a single, shared resource. But there are several things to keep in mind to make reading from a disk faster.</p>
<p>As a test case, we’ll look at parsing a sample of the comments from the <a href="http://blog.stackexchange.com/category/cc-wiki-dump/">Stack Exchange data dumps</a>. For each user, we’ll find the date range when that user posted comments to Stack Overflow. The data’s in XML, and if we were sane, we’d just use a lazy XML parser. But for the sake of demonstration, we’ll use regular expressions to pull out the attributes and their values for each comment.</p>
<p>For this problem, we’ll look at two ways of handling the file IO. First, we’ll consider using straightforward lazy, sequential access, and then we’ll look at memory mapping the file. We’ll look at the performance of each and consider what situations each might be useful in.</p>
<h3 id="getting-ready">Getting ready…</h3>
<p>For this, we’ll need a number of dependencies, so we’ll list these in our <a href="http://leiningen.org/">Leiningen</a> <a href="project.clj"><code>project.clj</code></a> file.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure"><span class="kw">:dependencies</span> [[org.clojure/clojure <span class="st">&quot;1.5.1&quot;</span>]
               [nio <span class="st">&quot;0.0.5&quot;</span>]
               [org.apache.commons/commons-lang3 <span class="st">&quot;3.1&quot;</span>]
               [clj-time <span class="st">&quot;0.5.0&quot;</span>]]</code></pre>
<p>Then, we’ll also have a number of imports and includes.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">import</span> &#39;[java.io File RandomAccessFile]
        &#39;[java.nio.charset Charset]
        &#39;[org.apache.commons.lang3 StringEscapeUtils])
(<span class="kw">use</span> &#39;[clj-time.core <span class="kw">:exclude</span> (<span class="kw">extend</span>)]
     &#39;[clj-time <span class="kw">format</span> coerce])
(<span class="kw">require</span> &#39;[nio.core <span class="kw">:as</span> nio]
         &#39;[clojure.java.io <span class="kw">:as</span> io]
         &#39;[clojure.string <span class="kw">:as</span> string]
         &#39;[clojure.core.reducers <span class="kw">:as</span> r])</code></pre>
<p>And for the data file, we can just use a sample of the Stack Exchange comments. You can find a (non-random) sample of 100,000 lines from that file in the source code for this chapter or download it from <a href="/clj-data-analysis/data/comments.xml">here</a>.</p>
<h3 id="how-to-do-it">How to do it…</h3>
<p>First, let’s tackle the problem domain. We’ll define a record that holds a user’s identifier and the range of comments seen so far. And we’ll write a function that will combine two <code>UserPost</code> records into one with the maximum range from both (assuming they represent the same user). Also, we’ll write a combiner function for a map of UserPost records.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defrecord</span><span class="fu"> UserPost</span>
  [user-id earliest-date latest-date])
(<span class="kw">defn</span><span class="fu"> combine-user-posts</span>
  [up1 up2]
  (UserPost.
    (<span class="kw">:user-id</span> up1)
    (<span class="kw">:earliest-date</span>
      (from-long (<span class="kw">min</span> (to-long (<span class="kw">:earliest-date</span> up1))
                      (to-long (<span class="kw">:earliest-date</span> up2)))))
    (<span class="kw">:latest-date</span>
      (from-long (<span class="kw">max</span> (to-long (<span class="kw">:latest-date</span> up1))
                      (to-long (<span class="kw">:latest-date</span> up2)))))))
(<span class="kw">def</span><span class="fu"> combiner</span>
  (r/monoid (<span class="kw">partial</span> <span class="kw">merge-with</span> combine-user-posts)
            <span class="kw">hash-map</span>))</code></pre>
<p>Next, we’ll add some general data parsing functions. The first is simply a lazy version of <code>clojure.string/split-lines</code>. The rest are more specifically targeted to the task at hand. They identify lines from the input that contain data, parse those into map, and eventually parse them into a UserPost.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> split-lines</span>
  ([input] (split-lines input <span class="dv">0</span>))
  ([input start]
   (<span class="kw">let</span> [end (.indexOf input <span class="dv">10</span> start)]
     (<span class="kw">when-not</span> (<span class="kw">=</span> end -<span class="dv">1</span>)
       (<span class="kw">lazy-seq</span>
         (<span class="kw">cons</span> (String.
                 (.trim (.substring input start end)))
               (split-lines input (<span class="kw">inc</span> end))))))))
(<span class="kw">defn</span><span class="fu"> data-line</span>?
  [line]
  (.startsWith (string/trim line) <span class="st">&quot;&lt;row &quot;</span>))
(<span class="kw">defn</span><span class="fu"> parse-pair</span>
  [[k v]]
  [(<span class="kw">keyword</span> k) (StringEscapeUtils/unescapeXml v)])
(<span class="kw">defn</span><span class="fu"> parse-line</span>
  [line]
  (<span class="kw">-&gt;&gt;</span> line
    (<span class="kw">re-seq</span> <span class="st">#&quot;(\w+)=</span>\&quot;<span class="st">([^</span>\&quot;<span class="st">]*)</span>\&quot;<span class="st">&quot;</span>)
    (<span class="kw">map</span> <span class="kw">next</span>)
    (<span class="kw">map</span> parse-pair)
    flatten
    (<span class="kw">apply</span> <span class="kw">hash-map</span>)))
(<span class="kw">def</span><span class="fu"> </span>^<span class="kw">:dynamic</span> *date-formatter*
  (formatters <span class="kw">:date-hour-minute-second-ms</span>))
(<span class="kw">defn</span><span class="fu"> line-&gt;user-post</span>
  [line]
  (<span class="kw">let</span> [user-id (<span class="kw">if-let</span> [uid (<span class="kw">:UserId</span> line)]
                  (<span class="kw">read-string</span> uid)
                  nil)
        cdate (parse *date-formatter*
                     (<span class="kw">:CreationDate</span> line))]
    (UserPost. user-id cdate cdate)))</code></pre>
<p>The next two functions process a sequence of lines into a map, which associates the users’ identifiers with the <code>UserPost</code> objects for each user.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> process-user-map</span>
  ([] {})
  ([user-post-map line]
   (<span class="kw">let</span> [user-post (line-&gt;user-post line)
         user-id (<span class="kw">:user-id</span> user-post)]
     (<span class="kw">assoc</span> user-post-map
            user-id
            (<span class="kw">if-let</span> [current (<span class="kw">get</span> user-post-map user-id)]
              (combine-user-posts current user-post)
              user-post)))))
(<span class="kw">defn</span><span class="fu"> process-lines</span>
  [lines]
  (<span class="kw">-&gt;&gt;</span>
    lines
    (r/map parse-line)
    (r/filter data-line?)
    (r/fold combiner process-user-map)))</code></pre>
<h4 id="reading-a-file-serially">Reading a File Serially</h4>
<p>With that basis, the function to read the file serially is quite simple.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> serial-process</span>
  [file-name]
  (<span class="kw">with-open</span> [reader (io/reader file-name)]
    (process-lines (<span class="kw">line-seq</span> reader))))</code></pre>
<h4 id="reading-from-a-memory-mapped-file">Reading from a Memory-Mapped File</h4>
<p>The other option is to read from a memory-mapped file. This allows us to treat a file’s contents as a byte array. For certain kinds of access, this can be very fast, but it takes a lot more set-up to get going.</p>
<p>First, we’ll have a couple of parameters: the character set for the input and a hint for the size of chunk to break the file into.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">def</span><span class="fu"> </span>^<span class="kw">:dynamic</span> *charset* (Charset/forName <span class="st">&quot;UTF-8&quot;</span>))
(<span class="kw">def</span><span class="fu"> </span>^<span class="kw">:dynamic</span> *chunk-size* (<span class="kw">*</span> <span class="dv">10</span> <span class="dv">1024</span> <span class="dv">1024</span>))</code></pre>
<p>With those, we’ll break the file into chunks by skipping through it and reading ahead until we get to the end of a line. Later, when we actually read the file, this will make sure that lines aren’t broken across chunks.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> get-chunk-offsets</span>
  [f pos offsets chunk-size]
  (<span class="kw">let</span> [skip-to (<span class="kw">+</span> pos chunk-size)]
    (<span class="kw">if</span> (<span class="kw">&gt;=</span> skip-to (.length f))
      (<span class="kw">conj</span> offsets (.length f))
      (<span class="kw">do</span>
        (.seek f skip-to)
        (<span class="kw">while</span> (<span class="kw">not=</span> (.<span class="kw">read</span> f) (<span class="kw">int</span> <span class="ch">\n</span>ewline)))
        (<span class="kw">let</span> [new-pos (.getFilePointer f)]
          (<span class="kw">recur</span> f new-pos (<span class="kw">conj</span> offsets new-pos)
                 chunk-size))))))
(<span class="kw">defn</span><span class="fu"> get-chunks</span>
  ([file-name] (get-chunks file-name *chunk-size*))
  ([file-name chunk-size]
   (<span class="kw">with-open</span> [f (RandomAccessFile. file-name <span class="st">&quot;r&quot;</span>)]
     (<span class="kw">doall</span>
       (<span class="kw">partition</span> <span class="dv">2</span> <span class="dv">1</span> (get-chunk-offsets
                        f <span class="dv">0</span> [<span class="dv">0</span>] chunk-size))))))</code></pre>
<p>And with those, we can memory map each chunk and read the lines out of it as a sequence.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> read-chunk</span>
  [channel [from to]]
  (<span class="kw">let</span> [chunk-mmap
        (.<span class="kw">map</span>
          channel
          java.nio.channels.FileChannel$MapMode/READ_ONLY
          from
          (<span class="kw">-</span> to from))
        decoder (.newDecoder *charset*)]
    (<span class="kw">doall</span> 
      (split-lines
        (<span class="kw">str</span> (.decode decoder chunk-mmap))))))</code></pre>
<p>These let us bring everything together similar to how we did with serial-process.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> mmap-process</span>
  [file-name]
  (<span class="kw">let</span> [chan (nio/channel file-name)]
    (<span class="kw">-&gt;&gt;</span>
      file-name
      get-chunks
      (r/mapcat (<span class="kw">partial</span> read-chunk chan))
      (r/map parse-line)
      (r/filter data-line?)
      (r/fold combiner process-user-map))))</code></pre>
<h4 id="performance">Performance</h4>
<p>Now we compare two methods’ performance. (Measured with the <a href="https://github.com/hugoduncan/criterium">criterium</a> library.)</p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Function</strong></th>
<th align="left"><strong>Mean Time</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>serial-process</code></td>
<td align="left">14.15 sec</td>
</tr>
<tr class="even">
<td align="left"><code>mmap-process</code></td>
<td align="left">15.35 sec</td>
</tr>
</tbody>
</table>
<h3 id="how-it-works">How it works…</h3>
<p>So for this, the serial process is better. It’s become a cliché to say that disk is the new tape, but there’s a lot of truth in that. If we can arrange processing so that we access the disk sequentially, as we do whenever we read one line at a time, we’ll get about the best performance we can.</p>
<p>If we must have random access, for a disk-based index for example, then we’re better off memory mapping the part of the file that we’re going to access.</p>
<p>The numbers above support these guidelines. As usual, the devil is in the details and exactly how our processing is going to happen.</p>
<hr />
<blockquote>
<p><em>This post is a literate programming file. Click on the <a href="index.clj">raw</a> link below—and the <a href="project.clj">project.clj</a> file linked to above—to download a version of this post that you can load directly into a Clojure REPL.</em> <!-- vim: set textwidth=58: --></p>
</blockquote>]]></summary>
</entry>
<entry>
    <title>Downloading Data in Parallel</title>
    <link href="http://www.ericrochester.com//pages/code/downloading-data-in-parallel/index.html" />
    <id>http://www.ericrochester.com//pages/code/downloading-data-in-parallel/index.html</id>
    <published>2013-04-29T13:04:02Z</published>
    <updated>2013-04-29T13:04:02Z</updated>
    <summary type="html"><![CDATA[<blockquote>
<p><em>This is a recipe that I wrote for the <a href="/pages/announcements/clj-data-analysis/index.html"><em>Clojure Data Analysis Cookbook</em></a>. However, it didn’t make it into the final book, so I’m sharing it with you today. If you like this, check out <a href="http://www.packtpub.com/clojure-data-analysis-cookbook/book">the book</a>.</em></p>
</blockquote>
<p>Sometimes when getting resources, we have to download them from many URLs. Doing that sequentially for one or two sources is fine, but if there are too many, we will really want to make better use of our Internet connection by downloading several at once.</p>
<p>This recipe does that. It chunks a sequence of URLs and downloads a block in parallel. It uses the <a href="http://neotyk.github.com/http.async.client/"><code>http.async.client</code></a> library to perform the download asynchronously, and we’ll simply manage how we trigger those jobs.</p>
<h3 id="getting-ready">Getting ready…</h3>
<p>First, we need to make sure that our <a href="http://leiningen.org/">Leiningen</a> <a href="project.clj"><code>project.clj</code></a> file lists the dependencies we’ll need:</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure"><span class="kw">:dependencies</span> [[org.clojure/clojure <span class="st">&quot;1.4.0&quot;</span>]
               [http.async.client <span class="st">&quot;0.4.5&quot;</span>]]</code></pre>
<p>And we need to use those in our script or REPL.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">require</span> &#39;[http.async.client <span class="kw">:as</span> http])
(<span class="kw">import</span> [java.net URL])</code></pre>
<p>For this example, we’ll download all of the ZIP files related to the <a href="http://www.who.int/whosis/mort/">World Health Organization’s mortality data</a>. Let’s bind those to the name <code>urls</code>.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">def</span><span class="fu"> urls </span>
  (<span class="kw">let</span> [who-ftp 
        (<span class="kw">str</span> <span class="st">&quot;http://www.who.int/whosis/database/&quot;</span>
             <span class="st">&quot;mort/download/ftp/&quot;</span>)]
    [(<span class="kw">str</span> who-ftp <span class="st">&quot;documentation.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;availability.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;country_codes.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;notes.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;Pop.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;morticd07.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;morticd08.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;morticd09.zip&quot;</span>)
     (<span class="kw">str</span> who-ftp <span class="st">&quot;morticd10.zip&quot;</span>)]))</code></pre>
<h3 id="how-to-do-it">How to do it…</h3>
<p>First, let’s set our default block size. We’ll do this using a dynamic variable so we can easily change it with the <code>binding</code> form.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">def</span><span class="fu"> </span>^<span class="kw">:dynamic</span> *block-size* <span class="dv">3</span>)</code></pre>
<p>Now, we want to be able to see what we’re doing, so let’s wrap the <code>http.async.client/GET</code> function in a function that prints the URL when we start and returns the URL and the download.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> get-verbose</span>
  <span class="st">&quot;This uses http.async.client to download a URL.&quot;</span>
  [client url]
  (<span class="kw">println</span> <span class="st">&quot;GET&quot;</span> url)
  [url (http/GET client url)])</code></pre>
<p>Next, let’s take the output of <code>get-verbose</code> and force the response. Since we don’t care about the response itself, we’ll throw most of it away and just return the status information. Because we’re curious, this will also print out information as it’s working.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> get-response</span>
  <span class="st">&quot;This forces the response to download and prints out</span>
<span class="st">  what&#39;s happening.&quot;</span>
  [[url resp]]
  (<span class="kw">println</span> <span class="st">&quot;awaiting&quot;</span> url)
  (http/await resp)
  (<span class="kw">println</span> <span class="st">&quot;done&quot;</span> url)
  (http/status resp))</code></pre>
<p>To see how this will work, let’s write a function to download all of the URLs sequentially. This will also serve as a baseline to see how much of a speed-up we will get.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> sequential</span>
  <span class="st">&quot;This downloads the resources sequentially.&quot;</span>
  []
  (<span class="kw">with-open</span> [client (http/create-client
                       <span class="kw">:follow-redirects</span> true)]
    (<span class="kw">doall</span>
      (<span class="kw">map</span> get-response
           (<span class="kw">map</span> (<span class="kw">partial</span> get-verbose client)
                urls)))))</code></pre>
<p>Now to process the blocks, we’ll partition the URLs using <code>partition-all</code> and use a new function, <code>get-block</code>, to force all the downloads in each block to complete before we move on to the next block.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> get-block</span>
  <span class="st">&quot;This forces a block of responses to download.&quot;</span>
  [block]
  (<span class="kw">doall</span> (<span class="kw">map</span> get-response block)))
(<span class="kw">defn</span><span class="fu"> async</span>
  <span class="st">&quot;This downloads the resources asynchronously.&quot;</span>
  []
  (<span class="kw">with-open</span> [client (http/create-client
                       <span class="kw">:follow-redirects</span> true)]
    (<span class="kw">doall</span>
      (<span class="kw">mapcat</span> get-block
              (partition-all
                *block-size*
                (<span class="kw">map</span>
                  (<span class="kw">partial</span> get-verbose client)
                  urls))))))</code></pre>
<p>Now we run this by simply calling async.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">user=&gt; (async)
GET http://www.who.int/whosis/database/mort/download/ftp/documentation.zip
…</code></pre>
<h3 id="how-it-works">How it works…</h3>
<p>First, we partition the URLs into blocks that will be downloaded in parallel. Because this process is IO-bound, we don’t have to worry about matching the number of CPUs on our machines. The function <code>get-block</code> then takes each block and forces <code>get-response</code> to complete the download and return.</p>
<p>Playing around with the block size shows some impressive speed-ups. Using a block size of five takes almost half the time as the serial version. Experiment with a small subset of your downloads to see what the optimal block size is for your network and the resources you’re interested in.</p>
<hr />
<blockquote>
<p><em>This post is a literate programming file. Click on the <a href="index.clj">raw</a> link below—and the <a href="project.clj">project.clj</a> file linked to above—to download a version of this post that you can load directly into a Clojure REPL.</em> <!-- vim: set textwidth=58: --></p>
</blockquote>]]></summary>
</entry>
<entry>
    <title>Aggregating Semantic Web Data</title>
    <link href="http://www.ericrochester.com//pages/code/aggregating-semantic-web-data/index.html" />
    <id>http://www.ericrochester.com//pages/code/aggregating-semantic-web-data/index.html</id>
    <published>2013-04-08T13:04:02Z</published>
    <updated>2013-04-08T13:04:02Z</updated>
    <summary type="html"><![CDATA[<blockquote>
<p><em>This is a recipe that I wrote for the <a href="/pages/announcements/clj-data-analysis/index.html"><em>Clojure Data Analysis Cookbook</em></a>. However, it didn’t make it into the final book, so I’m sharing it with you today. If you like this, check out <a href="http://www.packtpub.com/clojure-data-analysis-cookbook/book">the book</a>.</em></p>
</blockquote>
<p>One of the benefits of <a href="http://linkeddata.org/">linked data</a> is that it <em>is</em> linked. Data in one place points to data in another place, and the two integrate easily. However, although the links are explicit, we still have to bring the data together manually. Let’s see how to do that with Clojure.</p>
<h3 id="getting-ready">Getting ready</h3>
<p>We’ll first need to list the dependencies that we’ll need in our <a href="http://leiningen.org/">Leiningen</a> <a href="project.clj"><code>project.clj</code></a> file.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure"><span class="kw">:dependencies</span> [[org.clojure/clojure <span class="st">&quot;1.4.0&quot;</span>]
               [incanter/incanter <span class="st">&quot;1.4.1&quot;</span>]
               [edu.ucdenver.ccp/kr-sesame-core <span class="st">&quot;1.4.5&quot;</span>]
               [org.clojure/tools.logging <span class="st">&quot;0.2.4&quot;</span>]
               [org.slf4j/slf4j-simple <span class="st">&quot;1.7.2&quot;</span>]]</code></pre>
<p>And we’ll need to include them in our script or REPL.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">require</span> &#39;(clojure.java [io <span class="kw">:as</span> io]))
(<span class="kw">require</span> &#39;(clojure [xml <span class="kw">:as</span> xml] 
                   [pprint <span class="kw">:as</span> pp]
                   [zip <span class="kw">:as</span> zip]))
(<span class="kw">use</span> &#39;incanter.core
     &#39;edu.ucdenver.ccp.kr.kb
     &#39;edu.ucdenver.ccp.kr.rdf
     &#39;edu.ucdenver.ccp.kr.sparql
     &#39;edu.ucdenver.ccp.kr.sesame.kb
     &#39;clojure.<span class="kw">set</span>)
(<span class="kw">import</span> [java.io File]
        [java.net URL URLEncoder])</code></pre>
<p>We’ll also use the <a href="/clj-data-analysis/data/currencies.ttl">currencies.ttl</a> file.</p>
<h3 id="how-to-do-it">How to do it…</h3>
<p>For this, we’ll load data from the <code>currencies.ttl</code> file and from <a href="http://dbpedia.org/About">DBPedia</a> into the triple store. Because the triples in one references the triples in the other, the two datasets are automatically merged. Then we can query the triple store and get data from both of the original sources back out.</p>
<p>To make this happen, first we need some functions to set up the plumbing for working with RDF. These will create and initialize the triple store that we’ll need to use.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> kb-memstore</span>
  <span class="st">&quot;This creates a Sesame triple store in memory.&quot;</span>
  []
  (kb <span class="kw">:sesame-mem</span>))
(<span class="kw">def</span><span class="fu"> tele-ont </span><span class="st">&quot;http://telegraphis.net/ontology/&quot;</span>)
(<span class="kw">defn</span><span class="fu"> init-kb</span>
  <span class="st">&quot;This creates an in-memory knowledge base and</span>
<span class="st">  initializes it with a default set of namespaces.&quot;</span>
  [kb-store]
  (register-namespaces
    kb-store
    [[<span class="st">&quot;geographis&quot;</span> (<span class="kw">str</span> tele-ont 
                        <span class="st">&quot;geography/geography#&quot;</span>)]
     [<span class="st">&quot;code&quot;</span> (<span class="kw">str</span> tele-ont <span class="st">&quot;measurement/code#&quot;</span>)]
     [<span class="st">&quot;money&quot;</span> (<span class="kw">str</span> tele-ont <span class="st">&quot;money/money#&quot;</span>)]
     [<span class="st">&quot;owl&quot;</span> <span class="st">&quot;http://www.w3.org/2002/07/owl#&quot;</span>]
     [<span class="st">&quot;rdf&quot;</span> (<span class="kw">str</span> <span class="st">&quot;http://www.w3.org/&quot;</span>
                 <span class="st">&quot;1999/02/22-rdf-syntax-ns#&quot;</span>)]
     [<span class="st">&quot;xsd&quot;</span> <span class="st">&quot;http://www.w3.org/2001/XMLSchema#&quot;</span>]
     [<span class="st">&quot;currency&quot;</span> (<span class="kw">str</span> <span class="st">&quot;http://telegraphis.net/&quot;</span>
                      <span class="st">&quot;data/currencies/&quot;</span>)]
     [<span class="st">&quot;dbpedia&quot;</span> <span class="st">&quot;http://dbpedia.org/resource/&quot;</span>]
     [<span class="st">&quot;dbpedia-ont&quot;</span> <span class="st">&quot;http://dbpedia.org/ontology/&quot;</span>]
     [<span class="st">&quot;dbpedia-prop&quot;</span> <span class="st">&quot;http://dbpedia.org/property/&quot;</span>]
     [<span class="st">&quot;err&quot;</span> <span class="st">&quot;http://ericrochester.com/&quot;</span>]]))</code></pre>
<p>And we’ll use the following utilities later on.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> rekey</span>
  <span class="st">&quot;This just flips the arguments for </span>
<span class="st">  clojure.set/rename-keys to make it more</span>
<span class="st">  convenient.&quot;</span>
  ([k-map <span class="kw">map</span>]
   (<span class="kw">rename-keys</span> 
     (<span class="kw">select-keys</span> <span class="kw">map</span> (<span class="kw">keys</span> k-map)) k-map)))
(<span class="kw">defn</span><span class="fu"> binding-str</span>
  <span class="st">&quot;This takes a binding, pulls out the first tag&#39;s </span>
<span class="st">  content, and concatenates it into a string.&quot;</span>
  ([b]
   (<span class="kw">apply</span> <span class="kw">str</span> (<span class="kw">:content</span> (<span class="kw">first</span> (<span class="kw">:content</span> b))))))
(<span class="kw">defn</span><span class="fu"> result-seq</span>
  <span class="st">&quot;This takes the first result and returns a sequence </span>
<span class="st">  of this node, plus all the nodes to the right of it.&quot;</span>
  ([first-result]
   (<span class="kw">cons</span> (zip/node first-result)
         (zip/rights first-result))))</code></pre>
<p>These build the <a href="http://www.w3.org/TR/sparql11-overview/">SPARQL</a> query and create a URL out of them for querying <a href="http://dbpedia.org/About">DBPedia</a>. The last, <code>query-sparql-results</code> gets the results, parses them, and navigates the XML tree to get to the results.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> make-query</span>
  <span class="st">&quot;This creates a query that returns all the </span>
<span class="st">  triples related to a subject URI. It does </span>
<span class="st">  filter out non-English strings.&quot;</span>
  ([subject kb]
   (<span class="kw">binding</span> [*kb* kb
             *select-limit* <span class="dv">200</span>]
     (sparql-select-query
       (<span class="kw">list</span> (<span class="kw">list</span> subject &#39;?/p &#39;?/o)
             &#39;(<span class="kw">:or</span> (<span class="kw">:not</span> (<span class="kw">:isLiteral</span> ?/o))
                   (!= (<span class="kw">:datatype</span> ?/o) rdf/langString)
                   (<span class="kw">=</span> (<span class="kw">:lang</span> ?/o) [<span class="st">&quot;en&quot;</span>])))))))
(<span class="kw">defn</span><span class="fu"> make-query-uri</span>
  <span class="st">&quot;This constructs a URI for the query.&quot;</span>
  ([base-uri query]
   (URL. (<span class="kw">str</span> base-uri
              <span class="st">&quot;?format=&quot;</span> 
              (URLEncoder/encode <span class="st">&quot;text/xml&quot;</span>)
              <span class="st">&quot;&amp;query=&quot;</span> (URLEncoder/encode query)))))
(<span class="kw">defn</span><span class="fu"> query-sparql-results</span>
  <span class="st">&quot;This queries a SPARQL endpoint and returns a </span>
<span class="st">  sequence of result nodes.&quot;</span>
  ([sparql-uri subject kb]
   (<span class="kw">-&gt;&gt;</span>
     kb
     <span class="co">;; Build the URI query string.</span>
     (make-query subject)
     (make-query-uri sparql-uri)
     <span class="co">;; Get the results, parse the XML, and</span>
     <span class="co">;; return the zipper.</span>
     io/input-stream
     xml/parse
     zip/xml-zip
     <span class="co">;; Find the first child.</span>
     zip/down
     zip/right
     zip/down
     <span class="co">;; Convert all children into a sequence.</span>
     result-seq)))</code></pre>
<p>We’ll download the data we need from <a href="http://dbpedia.org/About">DBPedia</a> and insert it into the triple store alongside the RDF file’s data.</p>
<p>As part of this, we will split all URI strings into prefixes and resources. If each prefix has a namespace abbreviation defined for it in <code>init-kb</code> above, the abbreviation needs to be used, and that and the resource are converted into a symbol together. Otherwise, the URI as a whole is converted into a symbol. What does this look like?</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> split-symbol</span>
  <span class="st">&quot;This splits a string on an index and returns a symbol</span>
<span class="st">  created by using the first part as the namespace and the</span>
<span class="st">  second as the symbol.&quot;</span>
  ([kb string <span class="kw">index</span>]
     (<span class="kw">if-let</span> [ns-prefix (<span class="kw">get</span> (<span class="kw">:ns-map-to-short</span> kb)
                             (.substring string <span class="dv">0</span> <span class="kw">index</span>))]
       (<span class="kw">symbol</span> ns-prefix (.substring string <span class="kw">index</span>))
       (<span class="kw">symbol</span> string))))
(<span class="kw">defn</span><span class="fu"> str-to-ns</span>
  <span class="st">&quot;This maps a URI string to a ns and symbol, given the</span>
<span class="st">  namespaces registered in the KB.&quot;</span>
  ([uri-string] (str-to-ns *kb* uri-string))
  ([kb uri-string]
   (<span class="kw">let</span> [index-gens
         (<span class="kw">list</span> #(.lastIndexOf uri-string (<span class="kw">int</span> <span class="ch">\#</span>))
               #(.lastIndexOf uri-string (<span class="kw">int</span> <span class="ch">\/</span>)))]
     (<span class="kw">if-let</span> [<span class="kw">index</span>
              (<span class="kw">first</span> 
                (<span class="kw">filter</span> #(<span class="kw">&gt;</span> % -<span class="dv">1</span>) 
                        (<span class="kw">map</span> (<span class="kw">fn</span> [f] (f)) index-gens)))]
       (split-symbol kb uri-string (<span class="kw">inc</span> <span class="kw">index</span>))
       (<span class="kw">symbol</span> uri-string)))))</code></pre>
<p>Next, we’ll need to convert a variety of data types as encoded in the result XML into native Clojure types, the way the triple store interface wants to work with them. For that we’ll use a multimethod.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">def</span><span class="fu"> xmls </span><span class="st">&quot;http://www.w3.org/2001/XMLSchema#&quot;</span>)
(<span class="kw">defmulti</span><span class="fu"> from-xml</span>
  (<span class="kw">fn</span> [r] [(<span class="kw">:tag</span> r) (<span class="kw">:datatype</span> (<span class="kw">:attrs</span> r))]))
(<span class="kw">defmethod</span><span class="fu"> from-xml </span>[<span class="kw">:uri</span> nil] [r]
  (str-to-ns (<span class="kw">apply</span> <span class="kw">str</span> (<span class="kw">:content</span> r))))
(<span class="kw">defmethod</span><span class="fu"> from-xml </span>[<span class="kw">:literal</span> nil] [r]
  (<span class="kw">apply</span> <span class="kw">str</span> (<span class="kw">:content</span> r)))
(<span class="kw">defmethod</span><span class="fu"> from-xml </span>[<span class="kw">:literal</span> (<span class="kw">str</span> xmls &#39;int)] [r]
  (<span class="kw">read-string</span> (<span class="kw">apply</span> <span class="kw">str</span> (<span class="kw">:content</span> r))))
(<span class="kw">defmethod</span><span class="fu"> from-xml </span><span class="kw">:default</span> [r]
  (<span class="kw">apply</span> <span class="kw">str</span> (<span class="kw">:content</span> r)))</code></pre>
<p>Now we need a function to convert each result node into a vector triple. This will be used by a later function that loads the data from DBPedia into the triple store.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> result-to-triple</span>
  <span class="st">&quot;This converts a result node into a triple vector.&quot;</span>
  ([iri r]
   (<span class="kw">let</span> [{<span class="kw">:keys</span> [tag <span class="kw">attrs</span> <span class="kw">content</span>]} r
         [p o] <span class="kw">content</span>]
     [iri
      (str-to-ns (binding-str p))
      (from-xml (<span class="kw">first</span> (<span class="kw">:content</span> o)))])))
(<span class="kw">defn</span><span class="fu"> load-dbpedia</span>
  <span class="st">&quot;This loads data from dbpedia for a specific IRI into a</span>
<span class="st">  KB.&quot;</span>
  ([kb sparql-uri iri]
   (<span class="kw">binding</span> [*kb* kb]
     (<span class="kw">-&gt;&gt;</span>
       kb
       (query-sparql-results sparql-uri iri)
       (<span class="kw">map</span> #(result-to-triple iri %))
       (add-statements kb)))))</code></pre>
<p>We’ll define a function to pull the objects of all same-as statements out of an RDF query and load all statements for that URI from DBPedia.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> load-same-as</span>
  <span class="st">&quot;This takes the results of a query for owl:sameAs and</span>
<span class="st">  loads the object URIs into the triple store from</span>
<span class="st">  DBPedia.&quot;</span>
  ([kb [_ _ same-as]]
   (load-dbpedia kb <span class="st">&quot;http://dbpedia.org/sparql&quot;</span> same-as)
   kb))</code></pre>
<p>Finally, <code>aggregate-dataset</code> drives the whole thing. It takes the triple store, the datafile, a query to execute on the final results, and a mapping between SPARQL query parameters and keywords for the final result.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">defn</span><span class="fu"> aggregate-dataset</span>
  [t-store data-file q col-map]
  (<span class="kw">binding</span> [*kb* t-store]
    <span class="co">;;; Load primary data.</span>
    (load-rdf-file t-store (File. data-file))
    <span class="co">;;; Load associated data.</span>
    (<span class="kw">reduce</span> load-same-as
            t-store
            (query-rdf t-store nil &#39;owl/sameAs nil))
    <span class="co">;;; Query </span>
    (to-dataset (<span class="kw">map</span> (<span class="kw">partial</span> rekey col-map)
                     (query t-store q)))))</code></pre>
<p>Now let’s use all this to create the dataset. We’ll bind the parameters to names so we can refer to them more easily and then use them to call <code>aggregate-dataset</code>.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">def</span><span class="fu"> data-file</span>
  <span class="st">&quot;../../../clj-data-analysis/data/currencies.ttl&quot;</span>)
(<span class="kw">def</span><span class="fu"> col-map </span>{&#39;?/name <span class="kw">:fullname</span>
              &#39;?/iso <span class="kw">:iso</span>
              &#39;?/shortName <span class="kw">:name</span>
              &#39;?/symbol <span class="kw">:symbol</span>
              &#39;?/country <span class="kw">:country</span>
              &#39;?/minorName <span class="kw">:minor-name</span>
              &#39;?/minorExponent <span class="kw">:minor-exp</span>
              &#39;?/peggedWith <span class="kw">:pegged-with</span>
              &#39;?/usedBanknotes <span class="kw">:used-banknotes</span>
              &#39;?/usedCoins <span class="kw">:used-coins</span>})
(<span class="kw">def</span><span class="fu"> q </span>&#39;[[?/c rdf/type money/Currency]
         [?/c owl/sameAs ?/d]
         [?/c money/name ?/name]
         [?/c money/shortName ?/shortName]
         [?/c money/isoAlpha ?/iso]
         [?/c money/minorName ?/minorName]
         [?/c money/minorExponent ?/minorExponent]
         [<span class="kw">:optional</span>
          [[?/d dbpedia-prop/symbol ?/symbol]]]
         [<span class="kw">:optional</span>
          [[?/d dbpedia-ont/usingCountry ?/country]]]
         [<span class="kw">:optional</span>
          [[?/d dbpedia-prop/peggedWith ?/peggedWith]]]
         [<span class="kw">:optional</span>
          [[?/d dbpedia-prop/usedBanknotes
            ?/usedBanknotes]]]
         [<span class="kw">:optional</span>
          [[?/d dbpedia-prop/usedCoins ?/usedCoins]]]])</code></pre>
<p>Let’s put it all together.</p>
<pre class="sourceCode clojure"><code class="sourceCode clojure">user=&gt; (aggregate-dataset (init-kb (kb-memstore))
            data-file q col-map)
[<span class="kw">:used-coins</span> <span class="kw">:symbol</span> <span class="kw">:pegged-with</span> <span class="kw">:country</span> <span class="kw">:name</span> <span class="kw">:minor-exp</span> <span class="kw">:iso</span> <span class="kw">:minor-name</span> <span class="kw">:used-banknotes</span> <span class="kw">:fullname</span>]
[<span class="dv">2550</span> <span class="st">&quot;د.إ&quot;</span> <span class="st">&quot;U.S. dollar = 3.6725 dirhams&quot;</span> dbpedia/United_Arab_Emirates <span class="st">&quot;dirham&quot;</span> <span class="st">&quot;2&quot;</span> <span class="st">&quot;AED&quot;</span> <span class="st">&quot;fils&quot;</span> <span class="dv">9223372036854775807</span> <span class="st">&quot;United Arab Emirates dirham&quot;</span>]
[<span class="dv">1</span> <span class="st">&quot;Af  or Afs&quot;</span> nil dbpedia/United_States_dollar <span class="st">&quot;afghani&quot;</span> <span class="st">&quot;2&quot;</span> <span class="st">&quot;AFN&quot;</span> <span class="st">&quot;pul&quot;</span> <span class="dv">1</span> <span class="st">&quot;Afghan afghani&quot;</span>]
[nil dbpedia/Albanian_lek nil dbpedia/Albania <span class="st">&quot;lek&quot;</span> <span class="st">&quot;2&quot;</span> <span class="st">&quot;ALL&quot;</span> <span class="st">&quot;qindarkë&quot;</span> nil <span class="st">&quot;Albanian lek&quot;</span>]
[<span class="dv">102050100200500</span> nil nil dbpedia/Armenia <span class="st">&quot;dram&quot;</span> <span class="st">&quot;0&quot;</span> <span class="st">&quot;AMD&quot;</span> <span class="st">&quot;luma&quot;</span> <span class="dv">9223372036854775807</span> <span class="st">&quot;Armenian dram&quot;</span>]
…</code></pre>
<h3 id="how-it-works">How it works…</h3>
<p>Linked data is, well, linked. Basically, we took all the data we’re interested in and dumped it into one big database. We used the links in the data itself to drive this by following <em>same-as</em> relationships already encoded in the data. We just used our query to pull it all together.</p>
<p>As an aside, notice the multimethod <code>from-xml</code> that dispatches on the result node’s tag name and its datatype attribute. Currently, this handles strings, integers, and URIs. They are sufficient for this dataset. If we need more, though, we can add them easily.</p>
<p>Also, in the query, all the phrases that are pulled in from DBPedia are marked <code>:optional</code>. We don’t want the overall query to fail because any of them are missing, and we can’t mark them all optional as a group because we don’t want the optional phrases as a whole to fail if any one is missing.</p>
<hr />
<blockquote>
<p><em>This post is a literate programming file. Click on the <a href="index.clj">raw</a> link below—and the <a href="project.clj">project.clj</a> file linked to above—to download a version of this post that you can load directly into a Clojure REPL.</em> <!-- vim: set textwidth=58: --></p>
</blockquote>]]></summary>
</entry>
<entry>
    <title>Clojure Data Analysis Cookbook</title>
    <link href="http://www.ericrochester.com//pages/announcements/clj-data-analysis/index.html" />
    <id>http://www.ericrochester.com//pages/announcements/clj-data-analysis/index.html</id>
    <published>2013-03-27T05:00:00Z</published>
    <updated>2013-03-27T05:00:00Z</updated>
    <summary type="html"><![CDATA[<p><img class='bookframe' src='http://dgdsbygo8mp3h.cloudfront.net/sites/default/files/imagecache/productview_larger/2643OS_0.jpg'/></p>
<p>I’m pleased to announce the release of the <a href="http://www.packtpub.com/clojure-data-analysis-cookbook/book"><em>Clojure Data Analysis Cookbook</em></a>, written by me, and published by <a href="http://www.packtpub.com/">Packt Publishing</a>.</p>
<p>This book has practical recipes for every stage of the data analysis process:</p>
<ul>
<li>acquiring data,</li>
<li>cleaning it,</li>
<li>analyzing it,</li>
<li>displaying and graphing it, and</li>
<li>publishing it on the web.</li>
</ul>
<p>There’s also a chapter on statistics and one on machine learning, as well as the obligatory (for Clojure, anyway) chapters on parallelism and concurrency.</p>
<p>From the book’s blurb:</p>
<blockquote>
<p>Data is everywhere and it’s increasingly important to be able to gain insights that we can act on. Using Clojure for data analysis and collection, this book will show you how to gain fresh insights and perspectives from your data with an essential collection of practical, structured recipes.</p>
<p>“The Clojure Data Analysis Cookbook” presents recipes for every stage of the data analysis process. Whether scraping data off a web page, performing data mining, or creating graphs for the web, this book has something for the task at hand.</p>
<p>You’ll learn how to acquire data, clean it up, and transform it into useful graphs which can then be analyzed and published to the Internet. Coverage includes advanced topics like processing data concurrently, applying powerful statistical techniques like Bayesian modelling, and even data mining algorithms such as K-means clustering, neural networks, and association rules.</p>
</blockquote>
<p>I’ve enjoyed writing this. It’s been a fun journey, and the people I’ve been working with on this have been awesome. It will be nice to do something else for a change, though.</p>
<p>Over the next few weeks, I’ll post a few of the recipes that I started on, but which I had to cut for space or other reasons. They’ll give you a taste of what’s available for you in this book, so look for them in the upcoming weeks!</p>
<p>In the meantime, I’ve set up <a href="http://www.ericrochester.com/clj-data-analysis/">a page</a> with links to the datasets I use in the book.</p>]]></summary>
</entry>
<entry>
    <title>Linked Open Data at the Rare Book School</title>
    <link href="http://www.ericrochester.com//pages/semantic-web/linked-open-data-rbs/index.html" />
    <id>http://www.ericrochester.com//pages/semantic-web/linked-open-data-rbs/index.html</id>
    <published>2011-07-21T20:39:00Z</published>
    <updated>2011-07-21T20:39:00Z</updated>
    <summary type="html"><![CDATA[<p><em>This is cross posted at <a href="http://www.scholarslab.org/digital-libraries/introduction-to-linked-open-data-at-rare-books-school/">The Scholars’ Lab Blog</a>.</em></p>
<p>Yesterday, I was fortunate to be invited by <a href="http://www.engl.virginia.edu/faculty/stauffer_andrew.shtml">Andrew Stauffer</a> and <a href="http://nowviskie.org/">Bethany Nowviskie</a> to present at their <a href="http://www.rarebookschool.org/">Rare Book School</a> course, <a href="http://rarebookschool.org/courses/libraries/l65/">Digitizing the Historical Record</a>. I talked about <a href="http://en.wikipedia.org/wiki/Linked_Data">Linked Open Data</a> (LOD), and afterward, <a href="http://twitter.com/!/bluesaepe">Dana Wheeles</a> talked about the <a href="http://www.nines.org/">NINES</a> project and how they use RDF and LOD.</p>
<p>I tried to present a gentle, mostly non-technical introduction to LOD, with an example of it in action. Hopefully, this posting will be a 50,000 foot overview also.</p>
<h2 id="the-linked-open-data-universe">The Linked Open Data Universe</h2>
<p style='font-size: smaller; width: 300px; margin-left: auto; margin-right: auto; text-align: center;'>
<a href="http://lod-cloud.net/"> <img src="http://richard.cyganiak.de/2007/10/lod/lod-datasets_2010-09-22.png" alt="Linked Open Data cloud" height="195" width="300" /> </a> <em>Linking Open Data cloud diagram, by Richard Cyganiak and Anja Jentzsch. <a href="http://lod-cloud.net/">http://lod-cloud.net/</a></em>
</p>

<p>The first thing to know about LOD is that it’s everywhere. Look at the <a href="http://lod-cloud.net/">Linked Open Data cloud diagram</a> above. All of these institutions are publishing data that anyone can use, and their data references others’ data also.</p>
<!--more-->

<h2 id="linked-data-vs-open-data-vs-rdf-data">Linked Data vs Open Data vs RDF Data</h2>
<p>First we need to unpack the term <em>Linked Open Data</em>:</p>
<p><strong>Linked</strong> is an approach to data. You need to provide context for your data; you need to point to other’s data.</p>
<p><strong>Open</strong> is a policy. Your data is out there for others to look at and use; you explicitly give others this permission.</p>
<p><strong>Data</strong> is a technology and a set of standards. Your data is available using an RDF data model (usually) so computers can easily process it.</p>
<p><em>(See <a href="http://blogs.ecs.soton.ac.uk/webteam/2011/07/17/linked-data-vs-open-data-vs-rdf-data/">Christopher Gutteridge’s post</a> for more about this distinction.)</em></p>
<h2 id="five-stars">Five Stars</h2>
<p>Creating LOD can seem overwhelming. Where do you start? What do you have to do? It’s not an all or nothing proposition. You can take what you have, figure out how close you are to LOD, and work gradually toward making your information a full member of the LOD cloud. The LOD community talks about having four-star data or five-star data. Here are what the different stars denote:</p>
<ol style="list-style-type: decimal">
<li>You’ve released the data using any format under an <strong>open</strong> license that allows others to view and use your data;</li>
<li>You’ve released the data in a <strong>structured</strong> format so that some program can deal with it (e.g., Excel);</li>
<li>You’ve released the data in a <strong>non-proprietary</strong> format, like CVS;</li>
<li>You’ve used <strong>HTTP URIs</strong> (things you can type into your web browser’s location bar) to identify things in your data and made those URIs available on the web so others can point to your stuff;</li>
<li>You explicitly <strong>link</strong> your data to others’ data to provide context.</li>
</ol>
<p><em>(This is all over the web. <a href="http://lab.linkeddata.deri.ie/2010/star-scheme-by-example/">Michael Hausenblas’ explanation with examples</a> is a good starting point.)</em></p>
<h2 id="representing-knowledge">Representing Knowledge</h2>
<p>A large part of this is about representing knowledge so computers can easily process it. Often LOD is encoded using <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">Resource Description Framework (RDF)</a>. This provides a way to model information using a series of statements. Each statement has three parts: a subject, a predicate, and an object. Subjects and predicates <em>must</em> be URIs. Objects can be URIs (linked data) or data literals.</p>
<p>The predicates that you can use are grouped into <em>vocabularies</em>. Each vocabulary is used for a specific domain.</p>
<p>We’re getting abstract, so let’s ground this discussion by looking at a specific vocabulary and set of statements.</p>
<h3 id="friend-of-a-friend">Friend of a Friend</h3>
<p>For describing people, there’s a vocabulary standard called <a href="http://www.foaf-project.org/">Friend of a Friend (FOAF)</a>. I’ve used that on my web site to provide information about me. (The file on my website is in <a href="http://en.wikipedia.org/wiki/RDF/XML">RDF/XML</a>, which can be frightening. I’ve converted it to <a href="http://en.wikipedia.org/wiki/Turtle_(syntax%29),%20which%20we%20can%20walk%20through%20more%20easily.">Turtle</a></p>
<p>I’ll show you parts of it line-by-line.</p>
<p>(Ahem. Before we start, a disclaimer: I need to update my FOAF file. It doesn’t reflect best practices. The referencing URL isn’t quite the way it should be, and it uses deprecated FOAF predicates. That said, if you can ignore my dirty laundry, it still illustrates the points I want to make about the basic structure of RDF.)</p>
<p>First,</p>
<pre><code>@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .</code></pre>
<p>This just says that anywhere <code>foaf:</code> appears later, replace it with the URL <code>http://xmlns.com/foaf/0.1/</code>.</p>
<pre><code>[] a &lt;http://xmlns.com/foaf/0.1/Person&gt;;</code></pre>
<p>This is a statement. <code>[]</code> just means that it’s talking about the document itself, which in this case is a stand-in for me. The predicate here is <code>a</code>, which is a shortcut that’s used to tell what type of an object something is. In this case, it says that I’m a person, as FOAF defines it.</p>
<p>And because the line ends in a semicolon, the rest of the statements are also about me. Or more specifically, about <code>[]</code>.</p>
<pre><code>foaf:firstName &quot;Eric&quot;;
foaf:surname &quot;Rochester&quot;;
foaf:name &quot;Eric Rochester&quot;;
foaf:nick &quot;Eric&quot;;</code></pre>
<p>This set of statements still have the implied subject of me, and they use a series of predicates from FOAF. The object of each is a literal string, giving a value. Roughly this translates into four statements:</p>
<ul>
<li>Eric’s first name is “Eric.”</li>
<li>Eric’s given name is “Rochester.”</li>
<li>Eric’s full name is “Eric Rochester.”</li>
<li>Eric’s nickname is “Eric.”</li>
</ul>
<p>The next statement is a little different:</p>
<pre><code>foaf:workplaceHomepage &lt;http://www.scholarslab.org/&gt; .</code></pre>
<p>This final statement has a URI as the object. It represents this statement:</p>
<ul>
<li>Eric’s workplace’s home page is “<a href="http://www.scholarslab.org/">http://www.scholarslab.org/</a>”.</li>
</ul>
<p>If this was a little overwhelming, thank you for sticking around this far. Now here’s what you need to know about modeling information using RDF:</p>
<ol style="list-style-type: decimal">
<li>Everything is expressed as subject-predicate-object statements; and</li>
<li>Predicates are grouped into vocabularies.</li>
</ol>
<p>The rest is just details.</p>
<h2 id="linked-open-data-and-the-semantic-web">Linked Open Data and the Semantic Web</h2>
<p>During my presentation, someone pointed out that this all sounds a lot like the <a href="http://en.wikipedia.org/wiki/Semantic_web">Semantic Web</a>.</p>
<p>Yes, it does. LOD is the semantic web without the focus on understanding and focusing more on what we can do. Understanding may come later—or not—but in the meantime we can still do some pretty cool things.</p>
<h2 id="so-what">So What?</h2>
<p>The benefit of all this is that it provides another layer for the internet. You can use this information to augment your own services (e.g., Google augments their search results with RDF data about product reviews) or build services on top of this information.</p>
<p>If you’re curious for more or still aren’t convinced, visit the <a href="http://obd.jisc.ac.uk/">Open Bibliographic Data Guide</a>. They make a business case and articulate some use cases for LOD for libraries and other institutions.</p>
<h2 id="for-example">For Example</h2>
<p>Discussing LOD can get pretty abstract and pretty meta. To keep things grounded, I spent a few hours and threw together a quick demonstration of what you can do with LOD.</p>
<p>The Library of Congress’ <a href="http://chroniclingamerica.loc.gov/">Chronicling America</a> project exposes data about the newspapers in its archives using RDF. It’s five-star data, too. For example, to tell the geographic location that the papers covered, it links to both <a href="http://www.geonames.org/">GeoNames</a> and <a href="http://dbpedia.org/About">DBpedia</a>. The LoC doesn’t provide the coordinates of these cities, but because they express the places with a link, I can follow those and read the latitude and longitude from there.</p>
<p>I wrote a <a href="http://www.python.org/">Python</a> script that uses <a href="http://www.rdflib.net/">RDFlib</a> to read the data from the LoC and GeoNames and writes it out using <a href="http://en.wikipedia.org/wiki/Kml">KML</a>. You can view this file using Google Maps or Google Earth.</p>
<p>Here’s the results of one run of the script. (I randomly pick 100 newspapers from the LoC, so the results of each run is different.)</p>
<iframe width="425" height="350" frameborder="0" scrolling="no" marginheight="0" marginwidth="0" src="http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=https:%2F%2Fgithub.com%2Ferochest%2Floc-chronicling-map%2Fraw%2Fmaster%2Fdata%2Fnewspapers.kml&amp;aq=&amp;sll=38.063606,-78.505873&amp;sspn=0.011741,0.016093&amp;ie=UTF8&amp;t=h&amp;ll=34.64296,-115.5352&amp;spn=26.67204,84.64626&amp;output=embed"></iframe>
<p><br /><small><a href="http://maps.google.com/maps?f=q&amp;source=embed&amp;hl=en&amp;geocode=&amp;q=https:%2F%2Fgithub.com%2Ferochest%2Floc-chronicling-map%2Fraw%2Fmaster%2Fdata%2Fnewspapers.kml&amp;aq=&amp;sll=38.063606,-78.505873&amp;sspn=0.011741,0.016093&amp;ie=UTF8&amp;t=h&amp;ll=34.64296,-115.5352&amp;spn=26.67204,84.64626" style="color:0000FF;text-align:left">View Larger Map</a></small></p>
<p>You can find the source for this example on both Github and BitBucket:</p>
<ul>
<li><a href="https://github.com/erochest/loc-chronicling-map">https://github.com/erochest/loc-chronicling-map</a></li>
<li><a href="https://bitbucket.org/erochest/loc-chronicling-map/overview">https://bitbucket.org/erochest/loc-chronicling-map/overview</a></li>
</ul>
<h2 id="resources">Resources</h2>
<p>Throughout this post, I’ve tried to link to some resources. Here are a few more (not all of these will be appropriate to a novice):</p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Linked_Data">The Wikipedia page on linked data</a>.</li>
<li><a href="http://obd.jisc.ac.uk/">The Open Bibliographic Data Guide</a>, which provides rationales for LOD.</li>
<li><a href="http://linkddata.org/">A portal to LOD resources and tools</a>.</li>
<li><a href="http://www.w3.org/wiki/LinkedData">A portal maintained by the W3C</a>.</li>
<li><a href="http://lod-cloud.net/">The LOD cloud</a>.</li>
<li><a href="http://www.w3.org/DesignIssues/LinkedData.html">The four rules of LOD</a>.</li>
<li><a href="http://lab.linkeddata.deri.ie/2010/star-scheme-by-example/">The five stars</a>.</li>
<li><a href="http://blogs.ecs.soton.ac.uk/webteam/2011/07/17/linked-data-vs-open-data-vs-rdf-data/">Linked vs Open vs Data</a>.</li>
<li><a href="http://linkeddatabook.com/book">A book on publishing LOD on the internet</a>.</li>
</ul>]]></summary>
</entry>

</feed>
